import json
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

from constraint_parsing_test import ConstraintParsingExperiment

def test_llm_constraint_parsing_detailed():
    """è¯¦ç»†æµ‹è¯•LLMçº¦æŸè§£æåŠŸèƒ½ - æŒ‰å¤æ‚åº¦åˆ†æ"""
    print("ğŸ¤– è¯¦ç»†æµ‹è¯•LLMçº¦æŸè§£æåŠŸèƒ½...")
    print("ğŸ“Š æŒ‰å¤æ‚åº¦åˆ†ç±»åˆ†ææ€§èƒ½")
    
    # åŠ è½½ç”¨æˆ·çº¦æŸæ ·æœ¬æ•°æ®
    dataset_path = os.path.join(current_dir, "user_appliance_constraint_samples.json")
    with open(dataset_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    experiment = ConstraintParsingExperiment()
    results = {"detailed_parsing_results": [], "complexity_analysis": {}}
    
    # æŒ‰å¤æ‚åº¦åˆ†ç±»æµ‹è¯•
    complexity_results = {"simple": [], "moderate": [], "complex": []}
    
    constraint_samples = dataset["constraint_samples"]
    
    for i, sample in enumerate(constraint_samples):
        print(f"ğŸ”„ LLMè¯¦ç»†è§£æè¿›åº¦: {i+1}/{len(constraint_samples)}")
        
        try:
            predicted = experiment.parse_constraint_with_llm(sample["input"])
            f1_scores = experiment.calculate_f1_score(predicted, sample["ground_truth"])
            
            result = {
                "sample_id": sample["id"],
                "constraint_text": sample["input"],
                "predicted_parsing": predicted,
                "ground_truth": sample["ground_truth"],
                "f1_scores": f1_scores,
                "complexity_level": sample["ground_truth"]["complexity"]
            }
            
            results["detailed_parsing_results"].append(result)
            complexity_results[result["complexity_level"]].append(f1_scores["overall_f1"])
            
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥ {sample['id']}: {e}")
            continue
    
    # è®¡ç®—å„å¤æ‚åº¦å¹³å‡æ€§èƒ½
    for complexity, scores in complexity_results.items():
        if scores:
            avg_f1 = sum(scores) / len(scores)
            results["complexity_analysis"][complexity] = {
                "sample_count": len(scores),
                "avg_f1_score": round(avg_f1 * 100, 1),
                "score_distribution": scores[:10]  # å‰10ä¸ªæ ·æœ¬åˆ†æ•°
            }
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_file = "experiments/llm_constraint_parsing_detailed_analysis.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("âœ… LLMçº¦æŸè§£æè¯¦ç»†æµ‹è¯•å®Œæˆ")
    print(f"ğŸ“Š è¯¦ç»†åˆ†æç»“æœ: {output_file}")
    
    # æ‰“å°æ€§èƒ½æ‘˜è¦
    print("\nğŸ“ˆ æ€§èƒ½æ‘˜è¦:")
    for complexity, analysis in results["complexity_analysis"].items():
        print(f"   {complexity.capitalize()}: {analysis['avg_f1_score']}% F1-Score ({analysis['sample_count']} samples)")
    
    return results

if __name__ == "__main__":
    test_llm_constraint_parsing_detailed()
