#!/usr/bin/env python3
"""
åŸºäºè§„åˆ™çš„ä¼˜åŒ–ç³»ç»Ÿ - åªä¼˜åŒ–æ¯ä¸ªç”µå™¨ç¼–å·æœ€å°çš„äº‹ä»¶
æ¯ä¸ªå®¶åº­å•ç‹¬è®¡æ—¶ï¼Œä»¿ç…§gurobiçš„ä¿å­˜æ ¼å¼å’Œè´¹ç”¨è®¡ç®—æ–¹å¼
"""

import pandas as pd
import json
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List
import logging
from first_event_optimizer import FirstEventOptimizer

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RuleBasedProcessor:
    def __init__(self, tariff_config_path: str):
        self.optimizer = FirstEventOptimizer(tariff_config_path)
        logger.info("åŸºäºè§„åˆ™çš„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def get_all_houses(self, data_dir: str) -> Dict[str, List[str]]:
        """è·å–æ‰€æœ‰houseåˆ—è¡¨"""
        houses = {"Economy_7": [], "Economy_10": []}

        for tariff_type in ["Economy_7", "Economy_10"]:
            tariff_dir = os.path.join(data_dir, tariff_type)
            if os.path.exists(tariff_dir):
                house_dirs = [d for d in os.listdir(tariff_dir) if d.startswith('house')]
                houses[tariff_type] = sorted(house_dirs, key=lambda x: int(x.replace('house', '')))

        logger.info(f"æ‰¾åˆ°çš„house:")
        logger.info(f"  Economy_7: {len(houses['Economy_7'])} houses")
        logger.info(f"  Economy_10: {len(houses['Economy_10'])} houses")

        return houses

    def load_all_events(self, house_id: str) -> pd.DataFrame:
        """åŠ è½½æˆ¿å±‹çš„æ‰€æœ‰äº‹ä»¶æ•°æ® - ä»¿ç…§gurobiæ–¹å¼"""
        current_dir = os.getcwd()
        while not os.path.exists(os.path.join(current_dir, "output", "02_event_segments")):
            parent_dir = os.path.dirname(current_dir)
            if parent_dir == current_dir:
                break
            current_dir = parent_dir

        events_file = os.path.join(current_dir, "output", "02_event_segments", house_id, f"02_appliance_event_segments_id_{house_id}.csv")

        if not os.path.exists(events_file):
            raise FileNotFoundError(f"äº‹ä»¶æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {events_file}")

        events_df = pd.read_csv(events_file)
        events_df['start_time'] = pd.to_datetime(events_df['start_time'])
        events_df['end_time'] = pd.to_datetime(events_df['end_time'])

        return events_df

    def calculate_complete_costs(self, house_id: str, tariff_type: str, optimization_results: List[Dict]) -> Dict:
        """è®¡ç®—å®Œæ•´çš„è´¹ç”¨ï¼ˆæ‰€æœ‰äº‹ä»¶ï¼‰ - ä»¿ç…§gurobiæ–¹å¼"""
        logger.info(f"å¼€å§‹è®¡ç®—å®Œæ•´è´¹ç”¨: {house_id} ({tariff_type})")

        try:
            # åŠ è½½æ•°æ®
            power_df = self.optimizer.load_power_data(house_id)
            all_events_df = self.load_all_events(house_id)

            # åˆ›å»ºä¼˜åŒ–ç»“æœæ˜ å°„
            optimization_map = {}
            for opt_result in optimization_results:
                optimization_map[opt_result['event_id']] = opt_result

            # è®¡ç®—æ‰€æœ‰äº‹ä»¶çš„è´¹ç”¨
            all_event_costs = []
            total_original_cost = 0.0
            total_optimized_cost = 0.0

            processed_events = 0
            failed_events = 0

            for idx, event in all_events_df.iterrows():
                try:
                    # è·å–åŠŸç‡æ›²çº¿
                    power_profile = self.optimizer.get_event_power_profile(event, power_df)

                    if not power_profile:
                        failed_events += 1
                        continue

                    # è®¡ç®—åŸå§‹æˆæœ¬
                    original_cost = self.optimizer.calculate_event_cost(power_profile, tariff_type)
                    total_original_cost += original_cost

                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¼˜åŒ–ç»“æœï¼ˆåªæœ‰ç¼–å·æœ€å°çš„äº‹ä»¶æ‰ä¼šè¢«ä¼˜åŒ–ï¼‰
                    event_id = event['event_id']
                    if event_id in optimization_map:
                        # ç¬¬ä¸€äº‹ä»¶ï¼šä½¿ç”¨ä¼˜åŒ–åçš„æˆæœ¬
                        opt_result = optimization_map[event_id]
                        optimized_cost = opt_result['optimized_cost']
                        is_optimized = True
                        optimized_start = opt_result['optimized_start_time']
                        optimized_end = opt_result['optimized_end_time']
                        cost_savings = original_cost - optimized_cost
                        savings_percentage = (cost_savings / original_cost * 100) if original_cost > 0 else 0
                        event_type = "first_event_optimized"
                    else:
                        # å…¶ä»–äº‹ä»¶ï¼šæˆæœ¬ä¸å˜
                        optimized_cost = original_cost
                        is_optimized = False
                        optimized_start = event['start_time']
                        optimized_end = event['end_time']
                        cost_savings = 0.0
                        savings_percentage = 0.0

                        # åˆ¤æ–­äº‹ä»¶ç±»å‹
                        if event['is_reschedulable']:
                            event_type = "reschedulable_not_optimized"
                        else:
                            event_type = "non_reschedulable"

                    total_optimized_cost += optimized_cost

                    # å¤„ç†åˆ—åå·®å¼‚ï¼šå…¨äº‹ä»¶æ•°æ®ç”¨appliance_IDï¼Œè¿‡æ»¤æ•°æ®ç”¨appliance_id
                    appliance_id_value = event.get('appliance_ID', event.get('appliance_id', 'Unknown'))

                    all_event_costs.append({
                        'event_id': event['event_id'],
                        'appliance_name': event['appliance_name'],
                        'appliance_id': appliance_id_value,
                        'is_reschedulable': event['is_reschedulable'],
                        'is_optimized': is_optimized,
                        'event_type': event_type,
                        'original_start_time': event['start_time'].strftime('%Y-%m-%d %H:%M:%S'),
                        'original_end_time': event['end_time'].strftime('%Y-%m-%d %H:%M:%S'),
                        'optimized_start_time': optimized_start.strftime('%Y-%m-%d %H:%M:%S'),
                        'optimized_end_time': optimized_end.strftime('%Y-%m-%d %H:%M:%S'),
                        'duration_minutes': event['duration(min)'],
                        'original_cost': original_cost,
                        'optimized_cost': optimized_cost,
                        'cost_savings': cost_savings,
                        'savings_percentage': savings_percentage,
                        'power_profile_points': len(power_profile)
                    })

                    processed_events += 1

                except Exception as e:
                    logger.warning(f"å¤„ç†äº‹ä»¶ {event['event_id']} æ—¶å‡ºé”™: {e}")
                    failed_events += 1
                    continue

            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            total_savings = total_original_cost - total_optimized_cost
            overall_savings_percentage = (total_savings / total_original_cost * 100) if total_original_cost > 0 else 0

            result = {
                'house_id': house_id,
                'tariff_type': tariff_type,
                'total_events': len(all_event_costs),
                'optimized_events': len([e for e in all_event_costs if e['is_optimized']]),
                'total_original_cost': total_original_cost,
                'total_optimized_cost': total_optimized_cost,
                'total_savings': total_savings,
                'overall_savings_percentage': overall_savings_percentage,
                'all_event_costs': all_event_costs
            }

            return result

        except Exception as e:
            logger.error(f"è®¡ç®—å®Œæ•´è´¹ç”¨æ—¶å‡ºé”™: {e}")
            return None

    def save_results_gurobi_style(self, optimization_results: List[Dict], complete_result: Dict, house_id: str, tariff_type: str):
        """ä»¿ç…§gurobiæ ¼å¼ä¿å­˜ç»“æœ"""
        # åˆ›å»ºç»“æœç›®å½• - ä¿®æ­£ä¸ºoutputsï¼ˆå¤æ•°ï¼‰
        results_dir = os.path.join("outputs", tariff_type, house_id)
        os.makedirs(results_dir, exist_ok=True)

        # 1. ä¿å­˜ä¼˜åŒ–ç»“æœCSV (ä»¿ç…§gurobiæ ¼å¼)
        csv_data = []
        for result in optimization_results:
            csv_data.append({
                'event_id': result['event_id'],
                'appliance_name': result['appliance_name'],
                'appliance_id': result['appliance_id'],
                'original_start_time': result['original_start_time'].strftime('%Y-%m-%d %H:%M:%S'),
                'original_end_time': result['original_end_time'].strftime('%Y-%m-%d %H:%M:%S'),
                'optimized_start_time': result['optimized_start_time'].strftime('%Y-%m-%d %H:%M:%S'),
                'optimized_end_time': result['optimized_end_time'].strftime('%Y-%m-%d %H:%M:%S'),
                'duration_minutes': result['duration_minutes'],
                'original_cost': result['original_cost'],
                'optimized_cost': result['optimized_cost'],
                'cost_savings': result['cost_savings'],
                'savings_percentage': result['savings_percentage'],
                'is_shifted': result['is_shifted']
            })

        csv_file = os.path.join(results_dir, f"optimization_results_{house_id}_{tariff_type}.csv")
        csv_df = pd.DataFrame(csv_data)
        csv_df.to_csv(csv_file, index=False)

        # 2. ä¿å­˜è¯¦ç»†çš„æ‰€æœ‰äº‹ä»¶è´¹ç”¨CSV (ä»¿ç…§gurobiæ ¼å¼)
        csv_data = complete_result['all_event_costs']
        csv_df = pd.DataFrame(csv_data)
        csv_file = os.path.join(results_dir, f"complete_cost_analysis_{house_id}_{tariff_type}.csv")
        csv_df.to_csv(csv_file, index=False)

        # 3. ä¿å­˜æ±‡æ€»ç»Ÿè®¡JSON (ä»¿ç…§gurobiæ ¼å¼)
        summary = {k: v for k, v in complete_result.items() if k != 'all_event_costs'}
        summary['processing_timestamp'] = datetime.now().isoformat()

        json_file = os.path.join(results_dir, f"cost_summary_{house_id}_{tariff_type}.json")
        with open(json_file, 'w') as f:
            json.dump(summary, f, indent=2)

        logger.info(f"    ç»“æœå·²ä¿å­˜åˆ°: {results_dir} (ä»¿ç…§gurobiæ ¼å¼)")

    def save_optimization_results_only(self, optimization_results: List[Dict], house_id: str, tariff_type: str):
        """åªä¿å­˜ä¼˜åŒ–ç»“æœï¼Œä¸è®¡ç®—è´¹ç”¨"""
        # åˆ›å»ºç»“æœç›®å½• - ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        results_dir = os.path.join("/home/deep/TimeSeries/Agent_V2/experiments/BaselineComparison/rule_based/results", tariff_type, house_id)
        os.makedirs(results_dir, exist_ok=True)

        # ä¿å­˜ä¼˜åŒ–ç»“æœCSV
        csv_file = os.path.join(results_dir, f"optimization_results_{house_id}_{tariff_type}.csv")
        results_df = pd.DataFrame(optimization_results)
        results_df.to_csv(csv_file, index=False)

        # ä¿å­˜ç®€å•çš„æ±‡æ€»JSON
        summary = {
            'house_id': house_id,
            'tariff_type': tariff_type,
            'total_optimized_events': len(optimization_results),
            'optimization_timestamp': datetime.now().isoformat()
        }

        json_file = os.path.join(results_dir, f"optimization_summary_{house_id}_{tariff_type}.json")
        with open(json_file, 'w') as f:
            json.dump(summary, f, indent=2)

        logger.info(f"    ä¼˜åŒ–ç»“æœå·²ä¿å­˜åˆ°: {results_dir}")

    def process_single_house(self, house_id: str, tariff_type: str, data_dir: str) -> Dict:
        """å¤„ç†å•ä¸ªhouse - åªè¿›è¡Œäº‹ä»¶è¿ç§»ï¼Œä¸è®¡ç®—è´¹ç”¨"""
        house_start_time = time.time()
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†: {house_id} ({tariff_type}) - å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")

        try:
            # è¿è¡Œç¬¬ä¸€äº‹ä»¶ä¼˜åŒ–ï¼ˆäº‹ä»¶è¿ç§»ï¼‰
            csv_file = os.path.join(data_dir, tariff_type, house_id, f"tou_filtered_{house_id}_{tariff_type}.csv")

            logger.info(f"  æ­¥éª¤: ç¬¬ä¸€äº‹ä»¶ä¼˜åŒ–ï¼ˆäº‹ä»¶è¿ç§»ï¼‰...")
            optimization_start_time = time.time()
            optimization_result = self.optimizer.optimize_single_file(csv_file, house_id, tariff_type)
            optimization_time = time.time() - optimization_start_time

            if optimization_result["status"] != "success":
                logger.error(f"  ç¬¬ä¸€äº‹ä»¶ä¼˜åŒ–å¤±è´¥: {optimization_result['status']}")
                return {"status": "optimization_failed", "house_id": house_id, "tariff_type": tariff_type}

            # ä¿å­˜ç»“æœåˆ°æŒ‡å®šç›®å½•ç»“æ„
            logger.info(f"  æ­¥éª¤: ä¿å­˜ç»“æœ...")
            save_start_time = time.time()
            self.save_optimization_results_only(optimization_result['optimization_results'], house_id, tariff_type)
            save_time = time.time() - save_start_time

            # è®¡ç®—æ€»å¤„ç†æ—¶é—´
            total_processing_time = time.time() - house_start_time

            optimized_count = len(optimization_result['optimization_results'])
            logger.info(f"  âœ… {house_id} å¤„ç†å®Œæˆ: {optimized_count} ä¸ªäº‹ä»¶å·²è¿ç§»")
            logger.info(f"  â±ï¸ æ—¶é—´: ä¼˜åŒ– {optimization_time:.1f}s, ä¿å­˜ {save_time:.1f}s, æ€»è®¡ {total_processing_time:.1f}s")

            return {
                "status": "success",
                "house_id": house_id,
                "tariff_type": tariff_type,
                "optimized_events": optimized_count,
                "processing_time": total_processing_time
            }

        except Exception as e:
            logger.error(f"å¤„ç† {house_id} æ—¶å‡ºé”™: {e}")
            return {"status": "error", "house_id": house_id, "tariff_type": tariff_type, "error": str(e)}

    def run_batch_processing(self, data_dir: str):
        """è¿è¡Œæ‰¹é‡å¤„ç†"""
        logger.info("ğŸš€ å¼€å§‹åŸºäºè§„åˆ™çš„æ‰¹é‡ä¼˜åŒ–å¤„ç†")

        # è·å–æ‰€æœ‰house
        houses = self.get_all_houses(data_dir)

        all_results = []
        batch_start_time = time.time()

        # æŒ‰tariffç±»å‹å¤„ç†
        for tariff_type in ["Economy_7", "Economy_10"]:
            if not houses[tariff_type]:
                logger.info(f"è·³è¿‡ {tariff_type}: æ²¡æœ‰æ‰¾åˆ°house")
                continue

            logger.info(f"\nğŸ“Š å¼€å§‹å¤„ç† {tariff_type} ({len(houses[tariff_type])} houses)")
            tariff_start_time = time.time()

            for house_id in houses[tariff_type]:
                result = self.process_single_house(house_id, tariff_type, data_dir)
                all_results.append(result)

            tariff_time = time.time() - tariff_start_time
            logger.info(f"âœ… {tariff_type} å¤„ç†å®Œæˆï¼Œè€—æ—¶: {tariff_time//60:.0f}m {tariff_time%60:.1f}s")

        total_batch_time = time.time() - batch_start_time

        # ç»Ÿè®¡ç»“æœ
        successful_results = [r for r in all_results if r["status"] == "success"]
        failed_results = [r for r in all_results if r["status"] != "success"]

        logger.info(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
        logger.info(f"  æˆåŠŸ: {len(successful_results)} houses")
        logger.info(f"  å¤±è´¥: {len(failed_results)} houses")
        logger.info(f"  æ€»è€—æ—¶: {total_batch_time//3600:.0f}h {(total_batch_time%3600)//60:.0f}m {total_batch_time%60:.1f}s")

        if successful_results:
            total_optimized_events = sum(r["optimized_events"] for r in successful_results)
            logger.info(f"  æ€»ä¼˜åŒ–äº‹ä»¶æ•°: {total_optimized_events}")
            logger.info(f"  å¹³å‡æ¯ä¸ªhouseä¼˜åŒ–äº‹ä»¶æ•°: {total_optimized_events/len(successful_results):.1f}")

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    tariff_config_path = "../../../config/tariff_config.json"
    data_dir = "../flterted_data"

    # åˆ›å»ºå¤„ç†å™¨å¹¶è¿è¡Œå®Œæ•´æ‰¹é‡å¤„ç†
    processor = RuleBasedProcessor(tariff_config_path)
    processor.run_batch_processing(data_dir)

if __name__ == "__main__":
    main()