
# import os
# import pandas as pd
# import numpy as np

# # ï¼ˆ1ï¼‰è¯»å–æ•°æ®è·¯å¾„æ¥å£
# def load_power_data(file_path: str) -> pd.DataFrame:
#     df = pd.read_csv(file_path, parse_dates=["Time"])
#     return df

# # ï¼ˆ2ï¼‰å¼‚å¸¸è®°å½•å¤„ç†å‡½æ•°ï¼ˆå·²å¢å¼ºï¼‰
# def remove_issue_records(df: pd.DataFrame) -> pd.DataFrame:
#     if "Issues" in df.columns:
#         issues_count = (df["Issues"] == 1).sum()
#         print(f"âš ï¸ æ ‡è®°ä¸ºå¼‚å¸¸ (Issues == 1) çš„è®°å½•æ•°: {issues_count}")
#         df_clean = df[df["Issues"] != 1].copy()
#         print(f"âœ… æ¸…æ´—åæœ‰æ•ˆè®°å½•æ•°: {len(df_clean)}")
#         return df_clean
#     print("âš ï¸ æœªæ£€æµ‹åˆ° Issues åˆ—ï¼Œé»˜è®¤å…¨éƒ¨æœ‰æ•ˆ")
#     return df.copy()

# # ï¼ˆ3ï¼‰æ„ŸçŸ¥æ—¶é—´ç²’åº¦
# def detect_temporal_granularity(df: pd.DataFrame, time_col: str = "Time") -> str:
#     diffs = df[time_col].sort_values().diff().dropna()
#     if diffs.empty:
#         return "Unknown"
#     common_diff = diffs.mode()[0]
#     freq = pd.tseries.frequencies.to_offset(common_diff).freqstr.lower()
#     return f"1{freq}" if freq in {"s", "min", "h", "d"} else freq

# # ï¼ˆ4ï¼‰å¤„ç†é«˜é¢‘ï¼ˆ<1minï¼‰æ—¶é—´åºåˆ—æ•°æ®ï¼ˆèšåˆä¸ºåˆ†é’Ÿçº§ï¼‰
# def downsample_to_minute(df: pd.DataFrame) -> pd.DataFrame:
#     df["Time_min"] = df["Time"].dt.floor("min")
#     power_cols = ["Aggregate"] + [f"Appliance{i}" for i in range(1, 10)]
#     df_min = df.groupby("Time_min")[power_cols].mean().reset_index()
#     df_min.rename(columns={"Time_min": "Time"}, inplace=True)
#     return df_min

# # ï¼ˆ5ï¼‰å¤„ç†ä½é¢‘ï¼ˆ>1minï¼‰æ—¶é—´åºåˆ—æ•°æ®ï¼ˆæ’å€¼ï¼‰
# def interpolate_to_minute(df: pd.DataFrame, target_freq: str = "1min") -> pd.DataFrame:
#     df = df.set_index("Time").resample(target_freq).interpolate(method="time").reset_index()
#     return df

# # ï¼ˆ6ï¼‰ä¿å­˜ç»“æœå‡½æ•°ï¼ˆæ–°å¢æ‰“å°è¡Œæ•°ï¼‰
# def save_aligned_result(df: pd.DataFrame, save_dir: str, filename: str) -> str:
#     os.makedirs(save_dir, exist_ok=True)
#     save_path = os.path.join(save_dir, filename)
#     df.to_csv(save_path, index=False)
#     print(f"ğŸ“ å¯¹é½åæ€»è¡Œæ•°ï¼š{len(df)}")
#     return save_path

# # âœ… Agent ä¸»æµç¨‹å°è£…
# def preprocess_power_series(input_path: str="/home/deep/TimeSeries/dataset/cleand_data/CLEAN_House1.csv", 
#                             output_dir: str = "./output/01_preprocessed/",
#                             ) -> str:
#     print("æˆ‘ä»¬æ­£åœ¨æ‰§è¡Œæ—¶é—´åºåˆ—ç»†ç²’åº¦æ„ŸçŸ¥ä»»åŠ¡ï¼Œè¯·ç¨ç­‰ï¼š\n")
#     print("ğŸ¤– [Agent æ„ŸçŸ¥æ¨¡å—] å¯åŠ¨ï¼šè§£æåŸå§‹åŠŸç‡æ•°æ®ï¼Œæ‰§è¡Œæ—¶é—´ä¸€è‡´æ€§æ£€æŸ¥ä¸åˆ†è¾¨ç‡å¯¹é½...\n")

#     df_raw = load_power_data(input_path)
#     print(f"ğŸ“Š åŸå§‹è®°å½•æ•°ï¼š{len(df_raw)}")

#     df_clean = remove_issue_records(df_raw)

#     granularity = detect_temporal_granularity(df_clean)
#     print(f"ğŸ” æ„ŸçŸ¥æ—¶é—´ç²’åº¦ï¼š{granularity}")

#     if pd.Timedelta(granularity) < pd.Timedelta("1min"):
#         print("ğŸ“‰ é«˜é¢‘é‡‡æ ·æ•°æ® â†’ æ‰§è¡Œæ»‘åŠ¨å¹³å‡èšåˆ")
#         df_result = downsample_to_minute(df_clean)
#     elif pd.Timedelta(granularity) > pd.Timedelta("1min"):
#         print("ğŸ“ˆ ä½é¢‘é‡‡æ ·æ•°æ® â†’ æ‰§è¡Œæ’å€¼æ’è¡¥")
#         df_result = interpolate_to_minute(df_clean)
#     else:
#         print("â±ï¸ å·²ä¸º 1min ç²’åº¦ï¼Œæ— éœ€è°ƒæ•´")
#         df_result = df_clean.copy()

#     result_path = save_aligned_result(df_result, output_dir, "01_perception_alignment_result.csv")
#     print(f"\nâœ… å„ä¸ªç”µå™¨çš„åºåˆ—æ„ŸçŸ¥å¯¹é½å®Œæˆï¼æ–‡ä»¶å°†ä¿å­˜åœ¨è·¯å¾„ï¼š{result_path}")
#     print("ğŸ“Œ æˆ‘ç»™æ‚¨çœ‹ä¸€ä¸‹æ„ŸçŸ¥ç»“æœçš„å‰5è¡Œæ•°æ®ï¼Œå®ƒä»¬æ˜¯ä»¥1minä¸ºç»†ç²’åº¦çš„æ•°æ®ï¼š")
#     print(df_result.head())

#     return result_path
import os
import pandas as pd
import numpy as np

# (1) Load power data
def load_power_data(file_path: str) -> pd.DataFrame:
    df = pd.read_csv(file_path, parse_dates=["Time"])
    return df

# (2) Remove issue-labeled records (enhanced)
def remove_issue_records(df: pd.DataFrame) -> pd.DataFrame:
    if "Issues" in df.columns:
        issues_count = (df["Issues"] == 1).sum()
        print(f"âš ï¸ Number of records flagged as issues (Issues == 1): {issues_count}")
        df_clean = df[df["Issues"] != 1].copy()
        print(f"âœ… Number of valid records after cleaning: {len(df_clean)}")
        return df_clean
    print("âš ï¸ No 'Issues' column detected. Assuming all records are valid.")
    return df.copy()

# (3) Detect time granularity
def detect_temporal_granularity(df: pd.DataFrame, time_col: str = "Time") -> str:
    diffs = df[time_col].sort_values().diff().dropna()
    if diffs.empty:
        return "Unknown"
    common_diff = diffs.mode()[0]
    freq = pd.tseries.frequencies.to_offset(common_diff).freqstr.lower()
    return f"1{freq}" if freq in {"s", "min", "h", "d"} else freq

# (4) Downsample high-frequency (<1min) data to 1-minute resolution
def downsample_to_minute(df: pd.DataFrame) -> pd.DataFrame:
    df["Time_min"] = df["Time"].dt.floor("min")
    power_cols = ["Aggregate"] + [f"Appliance{i}" for i in range(1, 10)]
    df_min = df.groupby("Time_min")[power_cols].mean().reset_index()
    df_min.rename(columns={"Time_min": "Time"}, inplace=True)
    return df_min

# (5) Interpolate low-frequency (>1min) data to 1-minute resolution
def interpolate_to_minute(df: pd.DataFrame, target_freq: str = "1min") -> pd.DataFrame:
    df = df.set_index("Time").resample(target_freq).interpolate(method="time").reset_index()
    return df

# (6) Save results
def save_aligned_result(df: pd.DataFrame, save_dir: str, filename: str) -> str:
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, filename)
    df.to_csv(save_path, index=False)
    print(f"ğŸ“ Total rows after alignment: {len(df)}")
    return save_path

# âœ… Main agent preprocessing routine (single file)
def preprocess_power_series(input_path: str = "/home/deep/TimeSeries/dataset/cleand_data/CLEAN_House1.csv",
                            output_dir: str = "./output/01_preprocessed/") -> str:
    print("Starting time-series granularity recognition task...\n")
    print("ğŸ¤– [Agent Perception Module] Launching: parsing raw power data, checking time consistency and aligning resolution...\n")

    df_raw = load_power_data(input_path)
    print(f"ğŸ“Š Raw records count: {len(df_raw)}")

    df_clean = remove_issue_records(df_raw)

    granularity = detect_temporal_granularity(df_clean)
    print(f"ğŸ” Detected temporal granularity: {granularity}")

    if pd.Timedelta(granularity) < pd.Timedelta("1min"):
        print("ğŸ“‰ High-frequency sampling detected â†’ applying downsampling by averaging")
        df_result = downsample_to_minute(df_clean)
    elif pd.Timedelta(granularity) > pd.Timedelta("1min"):
        print("ğŸ“ˆ Low-frequency sampling detected â†’ applying interpolation")
        df_result = interpolate_to_minute(df_clean)
    else:
        print("â±ï¸ Already in 1-minute resolution, no adjustment needed.")
        df_result = df_clean.copy()

    result_path = save_aligned_result(df_result, output_dir, "01_perception_alignment_result.csv")
    print(f"\nâœ… Appliance-level series alignment completed! File saved at: {result_path}")
    print("ğŸ“Œ Here are the first 5 rows of the aligned 1-minute resolution data:")
    print(df_result.head())

    return result_path


# âœ… Enhanced batch processing function for multiple houses
def preprocess_power_series_single(input_path: str, house_id: str, base_output_dir: str = "./output/01_preprocessed/") -> str:
    """
    Process a single house data file with house-specific output directory.

    Args:
        input_path: Path to the input CSV file
        house_id: House identifier (e.g., "house1", "house2")
        base_output_dir: Base output directory

    Returns:
        Path to the saved result file
    """
    # Create house-specific output directory
    house_output_dir = os.path.join(base_output_dir, house_id)

    print(f"\nğŸ  Processing {house_id.upper()}...")
    print("ğŸ¤– [Agent Perception Module] Launching: parsing raw power data, checking time consistency and aligning resolution...\n")

    df_raw = load_power_data(input_path)
    print(f"ğŸ“Š Raw records count: {len(df_raw)}")

    df_clean = remove_issue_records(df_raw)

    granularity = detect_temporal_granularity(df_clean)
    print(f"ğŸ” Detected temporal granularity: {granularity}")

    if pd.Timedelta(granularity) < pd.Timedelta("1min"):
        print("ğŸ“‰ High-frequency sampling detected â†’ applying downsampling by averaging")
        df_result = downsample_to_minute(df_clean)
    elif pd.Timedelta(granularity) > pd.Timedelta("1min"):
        print("ğŸ“ˆ Low-frequency sampling detected â†’ applying interpolation")
        df_result = interpolate_to_minute(df_clean)
    else:
        print("â±ï¸ Already in 1-minute resolution, no adjustment needed.")
        df_result = df_clean.copy()

    # Save with house-specific filename
    filename = f"01_perception_alignment_result_{house_id}.csv"
    result_path = save_aligned_result(df_result, house_output_dir, filename)
    print(f"\nâœ… {house_id.upper()} appliance-level series alignment completed! File saved at: {result_path}")
    print("ğŸ“Œ Here are the first 5 rows of the aligned 1-minute resolution data:")
    print(df_result.head())

    return result_path


def batch_preprocess_power_series(input_dir: str = "/home/deep/TimeSeries/dataset/cleand_data/",
                                  base_output_dir: str = "./output/01_preprocessed/",
                                  file_pattern: str = "CLEAN_House*.csv") -> dict:
    """
    Batch process multiple house data files.

    Args:
        input_dir: Directory containing input CSV files
        base_output_dir: Base output directory
        file_pattern: File pattern to match (e.g., "CLEAN_House*.csv")

    Returns:
        Dictionary mapping house_id to result file path
    """
    import glob

    # Find all matching files
    search_pattern = os.path.join(input_dir, file_pattern)
    input_files = glob.glob(search_pattern)
    input_files.sort()  # Sort for consistent processing order

    if not input_files:
        print(f"âš ï¸ No files found matching pattern: {search_pattern}")
        return {}

    print(f"ğŸ” Found {len(input_files)} files to process:")
    for file_path in input_files:
        print(f"  - {os.path.basename(file_path)}")

    results = {}
    failed_files = []

    print(f"\nğŸš€ Starting batch processing of {len(input_files)} house data files...\n")
    print("=" * 80)

    for i, input_path in enumerate(input_files, 1):
        filename = os.path.basename(input_path)

        # Extract house number from filename (e.g., "CLEAN_House1.csv" -> "house1")
        try:
            if "House" in filename:
                house_num = filename.split("House")[1].split(".")[0]
                house_id = f"house{house_num}"
            else:
                # Fallback: use filename without extension
                house_id = os.path.splitext(filename)[0].lower()

            print(f"\n[{i}/{len(input_files)}] Processing: {filename} â†’ {house_id}")

            result_path = preprocess_power_series_single(input_path, house_id, base_output_dir)
            results[house_id] = result_path

            print(f"âœ… {house_id} completed successfully!")

        except Exception as e:
            print(f"âŒ Error processing {filename}: {str(e)}")
            failed_files.append(filename)
            continue

        print("-" * 80)

    # Summary
    print(f"\nğŸ‰ Batch processing completed!")
    print(f"âœ… Successfully processed: {len(results)} files")
    if failed_files:
        print(f"âŒ Failed to process: {len(failed_files)} files")
        for failed_file in failed_files:
            print(f"  - {failed_file}")

    print(f"\nğŸ“ Results saved in: {base_output_dir}")
    print("ğŸ“‹ Processed houses:")
    for house_id, result_path in results.items():
        print(f"  - {house_id}: {result_path}")

    return results


def batch_preprocess_specific_houses(house_numbers: list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 16, 17, 18, 19, 20, 21],
                                     input_dir: str = "/home/deep/TimeSeries/dataset/cleand_data/",
                                     base_output_dir: str = "./output/01_preprocessed/") -> dict:
    """
    Batch process specific house data files by house numbers.

    Args:
        house_numbers: List of house numbers to process
        input_dir: Directory containing input CSV files
        base_output_dir: Base output directory

    Returns:
        Dictionary mapping house_id to result file path
    """
    results = {}
    failed_files = []

    print(f"ğŸ” Target houses: {house_numbers}")
    print(f"ğŸ“ Input directory: {input_dir}")
    print(f"ğŸ“ Output directory: {base_output_dir}")

    # Check which files exist
    existing_files = []
    missing_files = []

    for house_num in house_numbers:
        filename = f"CLEAN_House{house_num}.csv"
        file_path = os.path.join(input_dir, filename)

        if os.path.exists(file_path):
            existing_files.append((house_num, file_path))
        else:
            missing_files.append(filename)

    if missing_files:
        print(f"âš ï¸ Missing files: {missing_files}")

    print(f"âœ… Found {len(existing_files)} files to process:")
    for house_num, file_path in existing_files:
        print(f"  - House{house_num}: {os.path.basename(file_path)}")

    if not existing_files:
        print("âŒ No files found to process!")
        return {}

    print(f"\nğŸš€ Starting batch processing of {len(existing_files)} house data files...\n")
    print("=" * 80)

    for i, (house_num, input_path) in enumerate(existing_files, 1):
        filename = os.path.basename(input_path)
        house_id = f"house{house_num}"

        try:
            print(f"\n[{i}/{len(existing_files)}] Processing: {filename} â†’ {house_id}")

            result_path = preprocess_power_series_single(input_path, house_id, base_output_dir)
            results[house_id] = result_path

            print(f"âœ… {house_id} completed successfully!")

        except Exception as e:
            print(f"âŒ Error processing {filename}: {str(e)}")
            failed_files.append(filename)
            continue

        print("-" * 80)

    # Summary
    print(f"\nğŸ‰ Batch processing completed!")
    print(f"âœ… Successfully processed: {len(results)} files")
    if failed_files:
        print(f"âŒ Failed to process: {len(failed_files)} files")
        for failed_file in failed_files:
            print(f"  - {failed_file}")

    print(f"\nğŸ“ Results saved in: {base_output_dir}")
    print("ğŸ“‹ Processed houses:")
    for house_id, result_path in results.items():
        print(f"  - {house_id}: {result_path}")

    return results


def batch_preprocess_specific_houses(house_numbers: list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 16, 17, 18, 19, 20, 21],
                                     input_dir: str = "/home/deep/TimeSeries/dataset/cleand_data/",
                                     base_output_dir: str = "./output/01_preprocessed/") -> dict:
    """
    Batch process specific house data files by house numbers.

    Args:
        house_numbers: List of house numbers to process
        input_dir: Directory containing input CSV files
        base_output_dir: Base output directory

    Returns:
        Dictionary mapping house_id to result file path
    """
    results = {}
    failed_files = []

    print(f"ğŸ” Target houses: {house_numbers}")
    print(f"ğŸ“ Input directory: {input_dir}")
    print(f"ğŸ“ Output directory: {base_output_dir}")

    # Check which files exist
    existing_files = []
    missing_files = []

    for house_num in house_numbers:
        filename = f"CLEAN_House{house_num}.csv"
        file_path = os.path.join(input_dir, filename)

        if os.path.exists(file_path):
            existing_files.append((house_num, file_path))
        else:
            missing_files.append(filename)

    if missing_files:
        print(f"âš ï¸ Missing files: {missing_files}")

    print(f"âœ… Found {len(existing_files)} files to process:")
    for house_num, file_path in existing_files:
        print(f"  - House{house_num}: {os.path.basename(file_path)}")

    if not existing_files:
        print("âŒ No files found to process!")
        return {}

    print(f"\nğŸš€ Starting batch processing of {len(existing_files)} house data files...\n")
    print("=" * 80)

    for i, (house_num, input_path) in enumerate(existing_files, 1):
        filename = os.path.basename(input_path)
        house_id = f"house{house_num}"

        try:
            print(f"\n[{i}/{len(existing_files)}] Processing: {filename} â†’ {house_id}")

            result_path = preprocess_power_series_single(input_path, house_id, base_output_dir)
            results[house_id] = result_path

            print(f"âœ… {house_id} completed successfully!")

        except Exception as e:
            print(f"âŒ Error processing {filename}: {str(e)}")
            failed_files.append(filename)
            continue

        print("-" * 80)

    # Summary
    print(f"\nğŸ‰ Batch processing completed!")
    print(f"âœ… Successfully processed: {len(results)} files")
    if failed_files:
        print(f"âŒ Failed to process: {len(failed_files)} files")
        for failed_file in failed_files:
            print(f"  - {failed_file}")

    print(f"\nğŸ“ Results saved in: {base_output_dir}")
    print("ğŸ“‹ Processed houses:")
    for house_id, result_path in results.items():
        print(f"  - {house_id}: {result_path}")

    return results
